{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=pd.read_csv('train.csv',header=None)\n",
    "data_train.columns=['class','title','content']\n",
    "data_test=pd.read_csv('test.csv',header=None)\n",
    "data_test.columns=['class','title','content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class      False\n",
       "title      False\n",
       "content    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_train = []\n",
    "for i in range(0,len(data_train['title'])):\n",
    "    A = data_train['content'][i].split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    A = [re_punc.sub('', w) for w in A]\n",
    "    # remove non-printatble characters\n",
    "    re_print = re.compile('[^%s]'%re.escape(string.printable))\n",
    "    A = [re_print.sub('', w) for w in A]\n",
    "    # convert to lower case\n",
    "    A = [word.lower() for word in A]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    A = [word for word in A if word.isalpha()]\n",
    "    # filter out stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    A = [w for w in A if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    A = [word for word in A if len(word) > 1]\n",
    "    A = \" \".join(A)\n",
    "    content_train.append(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_test = []\n",
    "for i in range(0,len(data_test['title'])):\n",
    "    A = data_test['content'][i].split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    A = [re_punc.sub('', w) for w in A]\n",
    "    # remove non-printatble characters\n",
    "    re_print = re.compile('[^%s]'%re.escape(string.printable))\n",
    "    A = [re_print.sub('', w) for w in A]\n",
    "    # convert to lower case\n",
    "    A = [word.lower() for word in A]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    A = [word for word in A if word.isalpha()]\n",
    "    # filter out stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    A = [w for w in A if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    A = [word for word in A if len(word) > 1]\n",
    "    A = \" \".join(A)\n",
    "    content_test.append(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_train = data_train['class']\n",
    "class_test = data_test['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding as a dict\n",
    "def load_embedding(filename):\n",
    "# load embedding into memory, skip first line\n",
    "    file = open(filename,'r')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "    # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "# total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = embedding.get(word)\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(content_train)\n",
    " \n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(content_train)\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in content_train])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "ytrain = to_categorical(class_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "encoded_docs = tokenizer.texts_to_sequences(content_test)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = to_categorical(class_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# train word2vec model\n",
    "model_train = Word2Vec(content_train, size=100, window=2, workers=4, min_count=2)\n",
    "    \n",
    "filename = 'embedding_word2vec.txt'\n",
    "model_train.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding('embedding_word2vec.txt')\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ranley/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 85, 100)           4979900   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 81, 128)           64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5120)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 25605     \n",
      "=================================================================\n",
      "Total params: 5,069,633\n",
      "Trainable params: 89,733\n",
      "Non-trainable params: 4,979,900\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/ranley/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      " - 12s - loss: 9.5815 - acc: 0.4001\n",
      "Epoch 2/10\n",
      " - 16s - loss: 9.5830 - acc: 0.4000\n",
      "Epoch 3/10\n",
      " - 16s - loss: 9.5830 - acc: 0.4000\n",
      "Epoch 4/10\n",
      " - 15s - loss: 9.5830 - acc: 0.4000\n",
      "Epoch 5/10\n",
      " - 16s - loss: 9.5830 - acc: 0.4000\n",
      "Epoch 6/10\n",
      " - 16s - loss: 9.5830 - acc: 0.4000\n",
      "Epoch 7/10\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
